input {
  # Application logs from file
  file {
    path => "/var/log/aiplatform/app.log"
    start_position => "beginning"
    codec => json
    tags => ["application", "aiplatform"]
  }

  # Kubernetes pod logs
  file {
    path => "/var/log/containers/*_aiplatform_*.log"
    start_position => "beginning"
    tags => ["kubernetes", "pod"]
    codec => json {
      charset => "UTF-8"
    }
  }

  # API access logs
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    tags => ["api", "nginx"]
  }

  # System logs
  file {
    path => "/var/log/syslog"
    start_position => "beginning"
    tags => ["system"]
  }

  # PostgreSQL logs
  file {
    path => "/var/log/postgresql/postgresql.log"
    start_position => "beginning"
    tags => ["database", "postgresql"]
  }

  # Redis logs
  file {
    path => "/var/log/redis/redis-server.log"
    start_position => "beginning"
    tags => ["cache", "redis"]
  }

  # Error logs
  file {
    path => "/var/log/aiplatform/error.log"
    start_position => "beginning"
    tags => ["error", "aiplatform"]
  }

  # TCP input for application direct logging
  tcp {
    port => 5000
    codec => json
    type => "tcp_json"
  }

  # UDP input for syslog
  udp {
    port => 5140
    type => "syslog"
  }
}

filter {
  # Parse JSON logs
  if [type] == "tcp_json" or [@metadata][original_format] == "json" {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Parse Nginx access logs
  if "nginx" in [tags] and [type] != "tcp_json" {
    grok {
      match => {
        "message" => "%{HTTPD_COMBINEDLOG}"
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }

    useragent {
      source => "agent"
      target => "useragent"
    }

    # Extract response time if available
    grok {
      match => {
        "message" => "response_time=%{NUMBER:response_time:float}"
      }
      allow_duplicates => false
    }
  }

  # Parse PostgreSQL logs
  if "postgresql" in [tags] {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{NUMBER:pid}\] %{LOGLEVEL:level}: %{GREEDYDATA:message}"
      }
    }

    # Extract query information
    grok {
      match => {
        "message" => "duration: %{NUMBER:query_duration:float}ms"
      }
      allow_duplicates => false
    }

    # Extract query type
    grok {
      match => {
        "message" => "statement: %{WORD:query_type} %{GREEDYDATA:query}"
      }
      allow_duplicates => false
    }
  }

  # Parse Kubernetes pod logs
  if "kubernetes" in [tags] {
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
    }

    # Extract pod metadata from path
    grok {
      match => {
        "path" => "/var/log/containers/%{DATA:pod_name}_%{DATA:namespace}_%{DATA:container_id}\.log"
      }
    }

    # Parse log level
    if [level] or [log] {
      mutate {
        add_field => { "log_level" => "%{level}" }
        add_field => { "log_message" => "%{log}" }
      }
    }
  }

  # Parse system syslog messages
  if "system" in [tags] or [type] == "syslog" {
    grok {
      match => {
        "message" => "%{SYSLOGLINE}"
      }
    }
  }

  # Add environment metadata
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "cluster" => "${CLUSTER_NAME:primary}"
      "region" => "${REGION:us-east-1}"
      "host_ip" => "%{HOSTNAME}"
    }

    # Ensure timestamp field exists
    add_field => {
      "@timestamp_log" => "%{@timestamp}"
    }
  }

  # Parse error levels
  if "error" in [tags] or [level] =~ /(?i:error|exception|fatal|critical)/ {
    mutate {
      add_tag => ["error_log"]
      add_field => {
        "severity" => "high"
      }
    }
  }

  # Parse warning levels
  if [level] =~ /(?i:warning|warn)/ {
    mutate {
      add_tag => ["warning_log"]
      add_field => {
        "severity" => "medium"
      }
    }
  }

  # Parse info levels
  if [level] =~ /(?i:info|information)/ {
    mutate {
      add_field => {
        "severity" => "low"
      }
    }
  }

  # GeoIP enrichment for API logs
  if "api" in [tags] and [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "message" ]
  }

  # Fingerprint for deduplication
  fingerprint {
    source => ["@timestamp", "hostname", "log_message"]
    target => "[@metadata][fingerprint]"
    method => "SHA1"
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    index => "aiplatform-logs-%{+YYYY.MM.dd}"
    document_id => "%{[@metadata][fingerprint]}"
    manage_template => true
    template_name => "aiplatform-logs"
    template_overwrite => true
    
    # Enable XPack if available
    username => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    ssl_enabled => "${ELASTICSEARCH_SSL:false}"
    ssl_verify_mode => "disable"
  }

  # Also send errors to a separate index
  if "error_log" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiplatform-errors-%{+YYYY.MM.dd}"
      username => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:changeme}"
      ssl_enabled => "${ELASTICSEARCH_SSL:false}"
    }
  }

  # Send metrics logs to metrics index
  if [type] == "tcp_json" and [metric] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiplatform-metrics-%{+YYYY.MM.dd}"
      username => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:changeme}"
      ssl_enabled => "${ELASTICSEARCH_SSL:false}"
    }
  }

  # Debug output
  if [@metadata][debug] {
    stdout {
      codec => json
    }
  }
}
